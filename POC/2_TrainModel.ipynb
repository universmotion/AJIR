{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model notebook\n",
    "\n",
    "Source: \n",
    "\n",
    "- https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "- https://huggingface.co/docs/setfit/how_to/zero_shot\n",
    "- : Generic model \n",
    "\n",
    "Dataset:\n",
    "\n",
    "- https://huggingface.co/docs/datasets/create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "ACRONYM = [\"YTA\", \"YWBTA\", \"NTA\", \"YWNBTA\", \"ESH\", \"NAH\", \"INFO\"]\n",
    "\n",
    "## Data enginerring\n",
    "data = pd.read_parquet(\"data/raw/comments-25-06-2024-14h30.parquet\")\n",
    "data = data[~data.body.isna()]\n",
    "\n",
    "data[\"label\"] = True # TODO: fake label\n",
    "data[\"label\"] = data[\"label\"].map(lambda x: np.random.randint(0, 2) == 1)\n",
    "\n",
    "def processing(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'http\\S+|www\\S+|https\\S+', '', doc, flags=re.MULTILINE)\n",
    "    # Suppression des mentions d'utilisateurs (par exemple, u/username)\n",
    "    doc = re.sub(r'u/\\S+', 'this people', doc)\n",
    "    # Suppression des caractères spéciaux et de la ponctuation\n",
    "    doc = re.sub(r'\\W', ' ', doc)\n",
    "    doc = re.sub(r'  ', ' ', doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://huggingface.co/blog/setfit\n",
    "\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "MODEL =\"sentence-transformers/paraphrase-albert-small-v2\" \n",
    "# \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "train_ds = Dataset.from_dict({\"text\": data[\"body\"].map(processing), \"label\": data[\"label\"]}, split=\"test\")\n",
    "test_ds = Dataset.from_dict({\"text\": data[\"body\"].map(processing), \"label\": data[\"label\"]}, split=\"train\")\n",
    "\n",
    "model = SetFitModel.from_pretrained(MODEL)\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_iterations=20, # Number of text pairs to generate for contrastive learning\n",
    "    num_epochs=1 # Number of epochs to use for contrastive learning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain/miniconda3/envs/projet/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# source: https://huggingface.co/docs/setfit/how_to/zero_shot\n",
    "\n",
    "# train_dataset = Dataset.from_dict({\"text\": data[\"body\"], \"label\": data[\"label\"]}, split=\"test\").with_format(\"torch\")\n",
    "# dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "# train_dataloader = DataLoader(dataloader, shuffle=True, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 9320\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 583\n",
      "  0%|          | 0/583 [00:00<?, ?it/s]\n",
      "  0%|          | 0/583 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2187, 'learning_rate': 3.3898305084745766e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "\u001b[A\u001b[A                                          \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2741, 'learning_rate': 1.694915254237288e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                             \n",
      "\n",
      "\u001b[A\u001b[A                                           \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2755, 'learning_rate': 1.8435114503816797e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
